{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This program opens a folder with all of ur papers in pdf format and then analyzes them\n",
    " - isnt put in a spreasheet format yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program below grabs all papers from the DB and prints them along with their contents and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: AND_ANALYSIS.txt\n",
      "Metadata: {'author': 'human'}\n",
      "File: Untitled_Paper.txt\n",
      "Metadata: {'author': 'human'}\n",
      "File: extracted-paper-test.txt\n",
      "Metadata: {'author': 'human'}\n",
      "File: extracted-paper-test2.txt\n",
      "Metadata: {'author': 'human'}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify your bucket name\n",
    "bucket_name = 'generated-research'  # Replace with your bucket name\n",
    "\n",
    "# List all objects in the bucket\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "# Check if the bucket contains objects\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        key = obj['Key']  # Object key (file name)\n",
    "\n",
    "        # Fetch metadata for each object\n",
    "        metadata_response = s3.head_object(Bucket=bucket_name, Key=key)\n",
    "        metadata = metadata_response['Metadata']\n",
    "\n",
    "        # Print file name and metadata\n",
    "        print(f\"File: {key}\")\n",
    "        print(\"Metadata:\", metadata)\n",
    "\n",
    "        # Download and print content of the object\n",
    "        obj_response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        # content = obj_response['Body'].read().decode('utf-8')  # Assuming the content is text\n",
    "        # print(f\"Content:\\n{content[:1000]}...\")  # Print the first 1000 characters for testing\n",
    "else:\n",
    "    print(\"Bucket is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ongoing program is for grabbing new PDFs of papers and putting them in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Title: AND ANALYSIS\n",
      "Extracted Text:\n",
      "THE DESIGN \n",
      "AND ANALYSIS \n",
      "OF \n",
      "COMPUTER \n",
      "ALGORITHMS \n",
      "Alfred V. Aho \n",
      "Bell Laboratories \n",
      "John E. Hopcroft \n",
      "Cornell University \n",
      "Jeffrey D. U II man \n",
      "Princeton University \n",
      "A \" Addison-W~ley Publishing Company \n",
      "Reading. Massachusetts · Menlo Park. California \n",
      "London · Amsterdam · Don Mills. Ontario · Sydney PREFACE \n",
      "\"' ...... \n",
      "The study of algorithms is at the very heart of c.:omputer science:- Irr; -redeht \n",
      "years a number of significant advances in the field of algorithms· have b'f'en \n",
      "made. These advances have ranged from the development off~s,te,r; algori,th~~. \n",
      "such as the fast Fourier transform. to the startling discovery of-q:rtain r)atur.~I. \n",
      "problems·for which all algorithms are inefficient. These results have kindled \n",
      "considerable interest in the study of algorithms. and the area of algorithm de­\n",
      "sign and analysis has blossomed into a field of intense interest. The intent of \n",
      "this book is to bring together the fundamental results in this area, so the uni­\n",
      "fying principles and underl...\n",
      "File 'AND_ANALYSIS.txt' successfully uploaded to bucket 'generated-research' with metadata author='human'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Function to extract text from a PDF URL\n",
    "def extract_text_from_pdf(url):\n",
    "    try:\n",
    "        # Send an HTTP request to the PDF URL\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Read the PDF content\n",
    "            pdf_file = BytesIO(response.content)\n",
    "            \n",
    "            # Initialize PDF reader\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            \n",
    "            # Extract text from all pages\n",
    "            text = ''\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "\n",
    "            return text\n",
    "        else:\n",
    "            print(f\"Failed to retrieve PDF from {url}, Status Code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while extracting PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate a title from the extracted PDF text\n",
    "def generate_title_from_text(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Use the first non-empty line as a potential title\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Assuming the title is capitalized or the first meaningful line\n",
    "        if line and len(line) > 10 and line.isupper():\n",
    "            return line\n",
    "\n",
    "    # Fallback if no title is found\n",
    "    return \"Untitled_Paper\"\n",
    "\n",
    "# Function to upload extracted text to S3 bucket with metadata\n",
    "def upload_to_s3(bucket_name, file_name, text_content):\n",
    "    try:\n",
    "        # Convert text to bytes (S3 requires byte input)\n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name, \n",
    "            Key=file_name, \n",
    "            Body=text_content, \n",
    "            Metadata={'author': 'human'}  # Adding metadata for the object\n",
    "        )\n",
    "        print(f\"File '{file_name}' successfully uploaded to bucket '{bucket_name}' with metadata author='human'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while uploading to S3: {e}\")\n",
    "\n",
    "# Example usage\n",
    "pdf_url = 'https://doc.lagout.org/science/0_Computer%20Science/2_Algorithms/The%20Design%20and%20Analysis%20of%20Computer%20Algorithms%20%5BAho,%20Hopcroft%20&%20Ullman%201974-01-11%5D.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_url)\n",
    "\n",
    "# Generate a title based on the content\n",
    "if pdf_text:\n",
    "    title = generate_title_from_text(pdf_text)\n",
    "    file_name = f\"{title}.txt\".replace(\" \", \"_\")  # Replace spaces with underscores for file name\n",
    "\n",
    "    print(f\"Generated Title: {title}\")\n",
    "    print(f\"Extracted Text:\\n{pdf_text[:1000]}...\")  # Print the first 1000 characters of the text\n",
    "    \n",
    "    # Upload to S3 with metadata\n",
    "    upload_to_s3('generated-research', file_name, pdf_text)\n",
    "else:\n",
    "    print(\"Failed to extract the PDF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'path/to/your/papers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/your/papers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m papers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'path/to/your/papers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import PyPDF2\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK resources if you don't have them\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "# def extract_text_from_pdf(pdf_path):\n",
    "#     text = \"\"\n",
    "#     with open(pdf_path, \"rb\") as file:\n",
    "#         reader = PyPDF2.PdfReader(file)\n",
    "#         for page_num in range(len(reader.pages)):\n",
    "#             text += reader.pages[page_num].extract_text()\n",
    "#     return text\n",
    "\n",
    "folder_path = 'path/to/your/papers'\n",
    "papers = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "            papers.append(f.read())\n",
    "\n",
    "# Function to process text and compute the required metrics\n",
    "def analyze_text(text, filename, csv_writer):\n",
    "    print(f\"Analyzing {filename}...\")\n",
    "    \n",
    "    # Split into sentences using nltk\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize words, and filter out punctuations\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # Get stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Sentence length: average number of words per sentence\n",
    "    sentence_lengths = [len(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "    avg_sentence_length = sum(sentence_lengths) / len(sentences) if sentences else 0\n",
    "\n",
    "    # Word length: average number of characters per word\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    avg_word_length = sum(word_lengths) / len(word_lengths) if word_lengths else 0\n",
    "\n",
    "    # Comma frequency (for compound sentences)\n",
    "    comma_count = text.count(',')\n",
    "\n",
    "    # Paragraph length: average number of sentences per paragraph\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newlines\n",
    "    paragraph_lengths = [len(nltk.sent_tokenize(para)) for para in paragraphs]\n",
    "    avg_paragraph_length = sum(paragraph_lengths) / len(paragraphs) if paragraphs else 0\n",
    "\n",
    "    # Punctuation frequency (exclamation marks and question marks)\n",
    "    punctuation_marks = re.findall(r'[!?]', text)\n",
    "    punctuation_count = len(punctuation_marks)\n",
    "\n",
    "    # Unique word count (proportional to length)\n",
    "    unique_words = set(words)\n",
    "    unique_word_proportion = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "    # Stop word frequency\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    stop_word_frequency = stop_word_count / len(words) if words else 0\n",
    "\n",
    "    # Top 10 words from Zipf's Law calculation\n",
    "    top_10_words = calculate_zipfs_law(words, filename)\n",
    "    \n",
    "    # Writing the statistics to the CSV file\n",
    "    csv_writer.writerow([\n",
    "        filename,\n",
    "        avg_sentence_length,\n",
    "        avg_word_length,\n",
    "        comma_count,\n",
    "        avg_paragraph_length,\n",
    "        punctuation_count,\n",
    "        unique_word_proportion,\n",
    "        stop_word_frequency,\n",
    "        top_10_words\n",
    "    ])\n",
    "\n",
    "# Function to calculate Zipf's Law\n",
    "def calculate_zipfs_law(words, filename):\n",
    "    # Calculate word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Sort words by frequency (highest to lowest)\n",
    "    sorted_word_freq = word_freq.most_common()\n",
    "    \n",
    "    # Rank the words by frequency\n",
    "    ranks = range(1, len(sorted_word_freq) + 1)\n",
    "    frequencies = [freq for word, freq in sorted_word_freq]\n",
    "    \n",
    "    # Plotting Zipf's Law (log-log plot of rank vs frequency)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.loglog(ranks, frequencies, marker=\"o\", linestyle=\"none\")\n",
    "    plt.title(f\"Zipf's Law: Word Frequency vs Rank ({filename})\")\n",
    "    plt.xlabel(\"Rank (log scale)\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot with the filename of the PDF\n",
    "    plt.savefig(f\"{filename}_zipf_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Return top 10 words as a string\n",
    "    top_10_words = ', '.join([f\"{word} ({freq})\" for word, freq in sorted_word_freq[:10]])\n",
    "    return top_10_words\n",
    "\n",
    "# Function to process all PDFs in a folder and output the results to a CSV\n",
    "def process_pdfs_in_folder(folder_path, csv_filename):\n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        # Write the CSV header\n",
    "        csv_writer.writerow([\n",
    "            \"Filename\",\n",
    "            \"Avg Sentence Length (words)\",\n",
    "            \"Avg Word Length (characters)\",\n",
    "            \"Comma Frequency\",\n",
    "            \"Avg Paragraph Length (sentences)\",\n",
    "            \"Punctuation Frequency (!, ?)\",\n",
    "            \"Unique Word Proportion\",\n",
    "            \"Stop Word Frequency\",\n",
    "            \"Top 10 Words (Zipf's Law)\"\n",
    "        ])\n",
    "        \n",
    "        # Iterate over each PDF in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(folder_path, filename)\n",
    "                text = extract_text_from_pdf(pdf_path)\n",
    "                analyze_text(text, filename, csv_writer)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = 'papers'  # Replace with the path to the folder containing the PDF files\n",
    "    csv_filename = 'analysis_results.csv'  # The name of the output CSV file\n",
    "    process_pdfs_in_folder(folder_path, csv_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
